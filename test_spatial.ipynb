{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting DLR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "from odc.stac import load\n",
    "import pystac_client\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cache directory\n",
    "CACHE_DIR = \"stac_data_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bbox(lon, lat, step=0.000001):\n",
    "    \"\"\"Create a bounding box around a point.\"\"\"\n",
    "    return [lon - step, lat - step, lon + step, lat + step]\n",
    "\n",
    "def get_stac_data_for_point(args):\n",
    "    \"\"\"Process a single point (for parallel execution)\"\"\"\n",
    "    catalog, collection, measurements, point_idx, lon, lat, bbox_step = args\n",
    "    \n",
    "    # Check if cached result exists\n",
    "    cache_file = f\"{CACHE_DIR}/point_{lon}_{lat}.parquet\"\n",
    "    if os.path.exists(cache_file):\n",
    "        try:\n",
    "            return pd.read_parquet(cache_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading cache file {cache_file}: {e}\")\n",
    "            pass  # If cache read fails, continue with regular processing\n",
    "    \n",
    "    try:\n",
    "        # Create bounding box for this point\n",
    "        bbox = create_bbox(lon, lat, bbox_step)\n",
    "        \n",
    "        # Search for items\n",
    "        search = catalog.search(\n",
    "            collections=collection,\n",
    "            bbox=bbox,\n",
    "            datetime=\"2018-03-01/2020-12-31\"\n",
    "        )\n",
    "        \n",
    "        # Convert search results to list\n",
    "        items = list(search.items())\n",
    "        \n",
    "        if len(items) > 0:\n",
    "            # Load the data\n",
    "            dataset = load(\n",
    "                items,\n",
    "                measurements=measurements,\n",
    "                bbox=bbox,\n",
    "                resolution=20\n",
    "            )\n",
    "            \n",
    "            # Convert to dataframe\n",
    "            data_point = dataset.isel(time=0).to_dataframe().reset_index()\n",
    "            \n",
    "            # Add point metadata\n",
    "            data_point['point_index'] = point_idx\n",
    "            data_point['source_lon'] = lon\n",
    "            data_point['source_lat'] = lat\n",
    "            \n",
    "            # Cache the result\n",
    "            try:\n",
    "                data_point.to_parquet(cache_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error caching point {point_idx}: {e}\")\n",
    "                pass  # If caching fails, continue anyway\n",
    "            \n",
    "            return data_point\n",
    "        else:\n",
    "            print(f\"No items found for point {point_idx} ({lon}, {lat})\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing point {point_idx}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_all_auxiliary_data(catalog, collection, measurements, long_lat, bbox_step=0.000001, max_workers=4):\n",
    "    \"\"\"Retrieve STAC data for all points using parallel processing.\"\"\"\n",
    "    \n",
    "    # Prepare arguments for each point\n",
    "    args_list = []\n",
    "\n",
    "    # Create argument list for all points\n",
    "    for i in range(len(long_lat)): \n",
    "        args_list.append((\n",
    "            catalog, \n",
    "            collection, \n",
    "            measurements, \n",
    "            i,  # Point index\n",
    "            long_lat.iloc[i]['GPS_LONG'],\n",
    "            long_lat.iloc[i]['GPS_LAT'],\n",
    "            bbox_step\n",
    "        ))\n",
    "    \n",
    "    # Process points in parallel\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks and track with progress bar\n",
    "        futures = [executor.submit(get_stac_data_for_point, args) for args in args_list]\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(args_list), desc=\"Processing points\"):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                results.append(result)\n",
    "    \n",
    "    # Combine all results\n",
    "    if not results:\n",
    "        print(\"No data retrieved!\")\n",
    "        return None\n",
    "    \n",
    "    points_df = pd.concat(results, ignore_index=True)\n",
    "    \n",
    "    # Create geometry points for GeoDataFrame\n",
    "    geometry_points = [Point(x, y) for x, y in zip(points_df['x'], points_df['y'])]\n",
    "    \n",
    "    # Convert to GeoDataFrame\n",
    "    points_gdf = gpd.GeoDataFrame(points_df, geometry=geometry_points, crs=3035)\n",
    "    \n",
    "    return points_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dlr_measurements = [\"MREF_B02\", \"MREF_B03\", \"MREF_B04\", \"MREF_B05\", \"MREF_B06\", \"MREF_B07\", \"MREF_B08\", \"MREF_B8A\", \"MREF_B11\", \"MREF_B12\",  # Mean Reflectance bands\n",
    "#                         \"MREF-STD_B02\", \"MREF-STD_B03\", \"MREF-STD_B04\", \"MREF-STD_B05\", \"MREF-STD_B06\", \"MREF-STD_B07\", \"MREF-STD_B08\", \"MREF-STD_B8A\", \"MREF-STD_B11\", \"MREF-STD_B12\",  # Standard deviation bands\n",
    "#                         \"SRC_B02\", \"SRC_B03\", \"SRC_B04\", \"SRC_B05\", \"SRC_B06\", \"SRC_B07\", \"SRC_B08\", \"SRC_B8A\", \"SRC_B11\", \"SRC_B12\",  # Bare Surface Reflectance bands\n",
    "#                         \"SRC-STD_B02\", \"SRC-STD_B03\", \"SRC-STD_B04\", \"SRC-STD_B05\", \"SRC-STD_B06\", \"SRC-STD_B07\", \"SRC-STD_B08\", \"SRC-STD_B8A\", \"SRC-STD_B11\", \"SRC-STD_B12\",  # Bare Surface Standard Deviation bands\n",
    "#                         \"SRC-CI95_B02\", \"SRC-CI95_B03\", \"SRC-CI95_B04\", \"SRC-CI95_B05\", \"SRC-CI95_B06\", \"SRC-CI95_B07\", \"SRC-CI95_B08\", \"SRC-CI95_B8A\", \"SRC-CI95_B11\", \"SRC-CI95_B12\",  # Bare Surface 95% Confidence Interval bands\n",
    "#                         \"SFREQ-BSF\" #, \"SFREQ-BSC\", \"SFREQ-VPC\"\n",
    "#                         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "target_raw = pd.read_csv('data/France_lab.csv')\n",
    "long_lat = target_raw[['GPS_LONG', 'GPS_LAT']]\n",
    "\n",
    "# Initialize STAC catalog\n",
    "dlr_catalog = pystac_client.Client.open(\"https://geoservice.dlr.de/eoc/ogc/stac/v1\")\n",
    "\n",
    "# Define measurements (you can reduce this list if you don't need all bands)\n",
    "dlr_measurements = [\"MREF_B02\", \"MREF_B03\", \"MREF_B04\", \"MREF_B08\", \"MREF_B11\", \"MREF_B12\"]\n",
    "\n",
    "# Define collection\n",
    "dlr_collection = [\"S2-soilsuite-europe-2018-2022-P5Y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing points:  18%|█▊        | 511/2807 [07:47<26:52,  1.42it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No items found for point 513 (88.888888, 88.888888)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing points:  33%|███▎      | 934/2807 [16:56<37:29,  1.20s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No items found for point 935 (88.888888, 88.888888)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing points: 100%|██████████| 2807/2807 [55:20<00:00,  1.18s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Process all points (consider using a subset for testing: long_lat.iloc[:10])\n",
    "results = get_all_auxiliary_data(\n",
    "    catalog=dlr_catalog,\n",
    "    collection=dlr_collection,\n",
    "    measurements=dlr_measurements,\n",
    "    long_lat=long_lat,\n",
    "    bbox_step=0.000001,\n",
    "    max_workers=4  # Adjust based on your CPU and bandwidth\n",
    ")\n",
    "\n",
    "# Save the results\n",
    "if results is not None:\n",
    "    results.to_file(\"data/auxiliary_data_results.gpkg\", driver=\"GPKG\")\n",
    "    results.to_parquet(\"data/auxiliary_data_results.parquet\")\n",
    "    print(\"Data saved successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
