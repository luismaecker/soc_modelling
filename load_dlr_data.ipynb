{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting DLR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "from odc.stac import load\n",
    "import pystac_client\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cache directory\n",
    "CACHE_DIR = \"data/stac_data_cache_full\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bbox(lon, lat, step=0.000001):\n",
    "    \"\"\"Create a bounding box around a point.\"\"\"\n",
    "    return [lon - step, lat - step, lon + step, lat + step]\n",
    "\n",
    "def get_stac_data_for_point(args):\n",
    "    \"\"\"Process a single point (for parallel execution)\"\"\"\n",
    "    catalog, collection, measurements, point_idx, lon, lat, bbox_step = args\n",
    "    \n",
    "    # Check if cached result exists\n",
    "    cache_file = f\"{CACHE_DIR}/point_{lon}_{lat}.parquet\"\n",
    "    if os.path.exists(cache_file):\n",
    "        try:\n",
    "            return pd.read_parquet(cache_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading cache file {cache_file}: {e}\")\n",
    "            pass  # If cache read fails, continue with regular processing\n",
    "    \n",
    "    try:\n",
    "        # Create bounding box for this point\n",
    "        bbox = create_bbox(lon, lat, bbox_step)\n",
    "        \n",
    "        # Search for items\n",
    "        search = catalog.search(\n",
    "            collections=collection,\n",
    "            bbox=bbox,\n",
    "            datetime=\"2018-03-01/2020-12-31\"\n",
    "        )\n",
    "        \n",
    "        # Convert search results to list\n",
    "        items = list(search.items())\n",
    "        \n",
    "        if len(items) > 0:\n",
    "            # Load the data\n",
    "            dataset = load(\n",
    "                items,\n",
    "                measurements=measurements,\n",
    "                bbox=bbox,\n",
    "                resolution=20\n",
    "            )\n",
    "            \n",
    "            # Convert to dataframe\n",
    "            data_point = dataset.isel(time=0).to_dataframe().reset_index()\n",
    "            \n",
    "            # Add point metadata\n",
    "            data_point['point_index'] = point_idx\n",
    "            data_point['source_lon'] = lon\n",
    "            data_point['source_lat'] = lat\n",
    "            \n",
    "            # Cache the result\n",
    "            try:\n",
    "                data_point.to_parquet(cache_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Error caching point {point_idx}: {e}\")\n",
    "                pass  # If caching fails, continue anyway\n",
    "            \n",
    "            return data_point\n",
    "        else:\n",
    "            print(f\"No items found for point {point_idx} ({lon}, {lat})\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing point {point_idx}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_all_auxiliary_data(catalog, collection, measurements, long_lat, bbox_step=0.000001, max_workers=4):\n",
    "    \"\"\"Retrieve STAC data for all points using parallel processing.\"\"\"\n",
    "    \n",
    "    # Prepare arguments for each point\n",
    "    args_list = []\n",
    "\n",
    "    # Create argument list for all points\n",
    "    for i in range(len(long_lat)): \n",
    "        args_list.append((\n",
    "            catalog, \n",
    "            collection, \n",
    "            measurements, \n",
    "            i,  # Point index\n",
    "            long_lat.iloc[i]['GPS_LONG'],\n",
    "            long_lat.iloc[i]['GPS_LAT'],\n",
    "            bbox_step\n",
    "        ))\n",
    "    \n",
    "    # Process points in parallel\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks and track with progress bar\n",
    "        futures = [executor.submit(get_stac_data_for_point, args) for args in args_list]\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(args_list), desc=\"Processing points\"):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                results.append(result)\n",
    "    \n",
    "    # Combine all results\n",
    "    if not results:\n",
    "        print(\"No data retrieved!\")\n",
    "        return None\n",
    "    \n",
    "    points_df = pd.concat(results, ignore_index=True)\n",
    "    \n",
    "    # Create geometry points for GeoDataFrame\n",
    "    geometry_points = [Point(x, y) for x, y in zip(points_df['x'], points_df['y'])]\n",
    "    \n",
    "    # Convert to GeoDataFrame\n",
    "    points_gdf = gpd.GeoDataFrame(points_df, geometry=geometry_points, crs=3035)\n",
    "    \n",
    "    return points_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlr_measurements = [\"MREF_B02\", \"MREF_B03\", \"MREF_B04\", \"MREF_B05\", \"MREF_B06\", \"MREF_B07\", \"MREF_B08\", \"MREF_B8A\", \"MREF_B11\", \"MREF_B12\",  # Mean Reflectance bands\n",
    "                        \"MREF-STD_B02\", \"MREF-STD_B03\", \"MREF-STD_B04\", \"MREF-STD_B05\", \"MREF-STD_B06\", \"MREF-STD_B07\", \"MREF-STD_B08\", \"MREF-STD_B8A\", \"MREF-STD_B11\", \"MREF-STD_B12\",  # Standard deviation bands\n",
    "                        \"SRC_B02\", \"SRC_B03\", \"SRC_B04\", \"SRC_B05\", \"SRC_B06\", \"SRC_B07\", \"SRC_B08\", \"SRC_B8A\", \"SRC_B11\", \"SRC_B12\",  # Bare Surface Reflectance bands\n",
    "                        \"SRC-STD_B02\", \"SRC-STD_B03\", \"SRC-STD_B04\", \"SRC-STD_B05\", \"SRC-STD_B06\", \"SRC-STD_B07\", \"SRC-STD_B08\", \"SRC-STD_B8A\", \"SRC-STD_B11\", \"SRC-STD_B12\",  # Bare Surface Standard Deviation bands\n",
    "                        \"SRC-CI95_B02\", \"SRC-CI95_B03\", \"SRC-CI95_B04\", \"SRC-CI95_B05\", \"SRC-CI95_B06\", \"SRC-CI95_B07\", \"SRC-CI95_B08\", \"SRC-CI95_B8A\", \"SRC-CI95_B11\", \"SRC-CI95_B12\",  # Bare Surface 95% Confidence Interval bands\n",
    "                        \"SFREQ-BSF\" #, \"SFREQ-BSC\", \"SFREQ-VPC\"\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "target_raw = pd.read_csv('data/France_lab.csv')\n",
    "long_lat = target_raw[['GPS_LONG', 'GPS_LAT']]\n",
    "\n",
    "# Initialize STAC catalog\n",
    "dlr_catalog = pystac_client.Client.open(\"https://geoservice.dlr.de/eoc/ogc/stac/v1\")\n",
    "\n",
    "# Define measurements (you can reduce this list if you don't need all bands)\n",
    "# dlr_measurements = [\"MREF_B02\", \"MREF_B03\", \"MREF_B04\", \"MREF_B08\", \"MREF_B11\", \"MREF_B12\"]\n",
    "\n",
    "# Define collection\n",
    "dlr_collection = [\"S2-soilsuite-europe-2018-2022-P5Y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing points:  15%|█▍        | 413/2807 [19:17<1:14:54,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing point 416: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing points:  18%|█▊        | 493/2807 [23:15<2:07:44,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing point 497: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing points:  18%|█▊        | 511/2807 [24:02<1:18:04,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No items found for point 513 (88.888888, 88.888888)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing points:  33%|███▎      | 933/2807 [43:10<1:02:52,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No items found for point 935 (88.888888, 88.888888)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing points:  59%|█████▉    | 1668/2807 [1:16:21<36:45,  1.94s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing point 1671: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing points:  78%|███████▊  | 2201/2807 [1:40:42<22:58,  2.27s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing point 2204: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing points:  86%|████████▌ | 2406/2807 [1:50:21<18:09,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing point 2410: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing points:  92%|█████████▏| 2582/2807 [1:58:41<14:03,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing point 2586: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing points: 100%|██████████| 2807/2807 [2:08:47<00:00,  2.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully\n"
     ]
    }
   ],
   "source": [
    "# # Process all points (consider using a subset for testing: long_lat.iloc[:10])\n",
    "# results = get_all_auxiliary_data(\n",
    "#     catalog=dlr_catalog,\n",
    "#     collection=dlr_collection,\n",
    "#     measurements=dlr_measurements,\n",
    "#     long_lat=long_lat,\n",
    "#     bbox_step=0.000001,\n",
    "#     max_workers=4  # Adjust based on your CPU and bandwidth\n",
    "# )\n",
    "\n",
    "# # Save the results\n",
    "# if results is not None:\n",
    "#     results.to_parquet(\"data/auxiliary_data_results_full.parquet\")\n",
    "#     print(\"Data saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_missing_dlr_data(\n",
    "    target_raw_path='data/France_lab.csv',\n",
    "    input_parquet=\"data/auxiliary_data_results_full.parquet\", \n",
    "    output_parquet=\"data/auxiliary_data_results_updated.parquet\", \n",
    "    cache_dir=\"data/stac_data_cache_full\",\n",
    "    max_workers=4,\n",
    "    max_retries=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Find and update missing DLR data points\n",
    "    \n",
    "    Args:\n",
    "        target_raw_path: Path to the CSV containing all target points (with GPS_LONG, GPS_LAT)\n",
    "        input_parquet: Path to the existing parquet file containing the processed results\n",
    "        output_parquet: Path to save the updated parquet file (if None, overwrites input_parquet)\n",
    "        cache_dir: Directory containing cached point data\n",
    "        max_workers: Maximum number of parallel workers\n",
    "        max_retries: Maximum number of retry attempts for each point\n",
    "        \n",
    "    Returns:\n",
    "        Updated GeoDataFrame with all available points\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import geopandas as gpd\n",
    "    import os\n",
    "    import time\n",
    "    import random\n",
    "    from shapely.geometry import Point\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    from tqdm import tqdm\n",
    "    import pystac_client\n",
    "    \n",
    "    if output_parquet is None:\n",
    "        output_parquet = input_parquet\n",
    "    \n",
    "    # Load original target points\n",
    "    print(f\"Loading original target points from {target_raw_path}\")\n",
    "    target_raw = pd.read_csv(target_raw_path)\n",
    "    original_points = target_raw[['GPS_LONG', 'GPS_LAT']]\n",
    "    \n",
    "    # Try to load existing results\n",
    "    if os.path.exists(input_parquet):\n",
    "        try:\n",
    "            print(f\"Loading existing results from {input_parquet}\")\n",
    "            existing_data = gpd.read_parquet(input_parquet)\n",
    "            print(f\"Loaded {len(existing_data)} points from existing data\")\n",
    "            \n",
    "            # Extract unique source coordinates from the existing data\n",
    "            processed_coords = set(zip(existing_data['source_lon'], existing_data['source_lat']))\n",
    "            print(f\"Found {len(processed_coords)} unique processed coordinates\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading existing results: {e}\")\n",
    "            existing_data = None\n",
    "            processed_coords = set()\n",
    "    else:\n",
    "        print(f\"No existing results found at {input_parquet}\")\n",
    "        existing_data = None\n",
    "        processed_coords = set()\n",
    "    \n",
    "    # Identify missing points\n",
    "    missing_points = []\n",
    "    for idx, row in original_points.iterrows():\n",
    "        point_coord = (row['GPS_LONG'], row['GPS_LAT'])\n",
    "        if point_coord not in processed_coords:\n",
    "            missing_points.append((idx, point_coord[0], point_coord[1]))\n",
    "    \n",
    "    print(f\"Found {len(missing_points)} missing points out of {len(original_points)} total points\")\n",
    "    \n",
    "    if not missing_points:\n",
    "        print(\"No missing points to process!\")\n",
    "        return existing_data\n",
    "    \n",
    "    # Initialize STAC catalog\n",
    "    dlr_catalog = pystac_client.Client.open(\"https://geoservice.dlr.de/eoc/ogc/stac/v1\")\n",
    "    \n",
    "    # Define collection\n",
    "    dlr_collection = [\"S2-soilsuite-europe-2018-2022-P5Y\"]\n",
    "    \n",
    "    # Define measurements\n",
    "    dlr_measurements = [\n",
    "        \"MREF_B02\", \"MREF_B03\", \"MREF_B04\", \"MREF_B05\", \"MREF_B06\", \"MREF_B07\", \"MREF_B08\", \"MREF_B8A\", \"MREF_B11\", \"MREF_B12\",  # Mean Reflectance bands\n",
    "        \"MREF-STD_B02\", \"MREF-STD_B03\", \"MREF-STD_B04\", \"MREF-STD_B05\", \"MREF-STD_B06\", \"MREF-STD_B07\", \"MREF-STD_B08\", \"MREF-STD_B8A\", \"MREF-STD_B11\", \"MREF-STD_B12\",  # Standard deviation bands\n",
    "        \"SRC_B02\", \"SRC_B03\", \"SRC_B04\", \"SRC_B05\", \"SRC_B06\", \"SRC_B07\", \"SRC_B08\", \"SRC_B8A\", \"SRC_B11\", \"SRC_B12\",  # Bare Surface Reflectance bands\n",
    "        \"SRC-STD_B02\", \"SRC-STD_B03\", \"SRC-STD_B04\", \"SRC-STD_B05\", \"SRC-STD_B06\", \"SRC-STD_B07\", \"SRC-STD_B08\", \"SRC-STD_B8A\", \"SRC-STD_B11\", \"SRC-STD_B12\",  # Bare Surface Standard Deviation bands\n",
    "        \"SRC-CI95_B02\", \"SRC-CI95_B03\", \"SRC-CI95_B04\", \"SRC-CI95_B05\", \"SRC-CI95_B06\", \"SRC-CI95_B07\", \"SRC-CI95_B08\", \"SRC-CI95_B8A\", \"SRC-CI95_B11\", \"SRC-CI95_B12\",  # Bare Surface 95% Confidence Interval bands\n",
    "        \"SFREQ-BSF\"  # Surface Frequency - Bare Soil Frequency\n",
    "    ]\n",
    "\n",
    "    # Create cache directory if it doesn't exist\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    # Function to process a single point with retries\n",
    "    def process_point_with_retries(point_info):\n",
    "        idx, lon, lat = point_info\n",
    "        \n",
    "        for retry in range(max_retries):\n",
    "            try:\n",
    "                # Create arguments for the get_stac_data_for_point function\n",
    "                args = (\n",
    "                    dlr_catalog,\n",
    "                    dlr_collection,\n",
    "                    dlr_measurements,\n",
    "                    idx,\n",
    "                    lon,\n",
    "                    lat,\n",
    "                    0.000001  # bbox_step\n",
    "                )\n",
    "                \n",
    "                result = get_stac_data_for_point(args)\n",
    "                if result is not None:\n",
    "                    print(f\"Successfully processed point {idx} ({lon}, {lat})\")\n",
    "                    return result\n",
    "                \n",
    "                print(f\"No data found for point {idx} ({lon}, {lat}) - Attempt {retry+1}/{max_retries}\")\n",
    "                \n",
    "                # Add randomized delay between retries\n",
    "                if retry < max_retries - 1:\n",
    "                    sleep_time = 5 + random.random() * 10\n",
    "                    print(f\"Waiting {sleep_time:.1f} seconds before retry...\")\n",
    "                    time.sleep(sleep_time)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing point {idx} ({lon}, {lat}) - Attempt {retry+1}/{max_retries}: {e}\")\n",
    "                \n",
    "                if retry < max_retries - 1:\n",
    "                    sleep_time = 5 + random.random() * 10\n",
    "                    print(f\"Waiting {sleep_time:.1f} seconds before retry...\")\n",
    "                    time.sleep(sleep_time)\n",
    "        \n",
    "        print(f\"Failed to process point {idx} ({lon}, {lat}) after {max_retries} attempts\")\n",
    "        return None\n",
    "    \n",
    "    # Process missing points in parallel\n",
    "    new_results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks and track with progress bar\n",
    "        futures = [executor.submit(process_point_with_retries, point) for point in missing_points]\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(missing_points), desc=\"Processing missing points\"):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                new_results.append(result)\n",
    "    \n",
    "    print(f\"Successfully processed {len(new_results)} out of {len(missing_points)} missing points\")\n",
    "    \n",
    "    # Combine with existing results\n",
    "    if new_results:\n",
    "        # Combine all new results\n",
    "        new_points_df = pd.concat(new_results, ignore_index=True)\n",
    "        \n",
    "        # Create geometry points for GeoDataFrame\n",
    "        geometry_points = [Point(x, y) for x, y in zip(new_points_df['x'], new_points_df['y'])]\n",
    "        \n",
    "        # Convert to GeoDataFrame\n",
    "        new_points_gdf = gpd.GeoDataFrame(new_points_df, geometry=geometry_points, crs=3035)\n",
    "        \n",
    "        # Combine with existing data if available\n",
    "        if existing_data is not None:\n",
    "            combined_gdf = pd.concat([existing_data, new_points_gdf], ignore_index=True)\n",
    "        else:\n",
    "            combined_gdf = new_points_gdf\n",
    "        \n",
    "        # Save the updated results\n",
    "        combined_gdf.to_parquet(output_parquet)\n",
    "        print(f\"Updated data saved to {output_parquet} ({len(combined_gdf)} total points)\")\n",
    "        \n",
    "        return combined_gdf\n",
    "    else:\n",
    "        print(\"No new data to add\")\n",
    "        return existing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading original target points from data/France_lab.csv\n",
      "Loading existing results from data/auxiliary_data_results_full.parquet\n",
      "Loaded 2799 points from existing data\n",
      "Found 2797 unique processed coordinates\n",
      "Found 8 missing points out of 2807 total points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing missing points:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed point 497 (0.57576, 46.423743)Successfully processed point 416 (-1.278976, 46.370656)\n",
      "\n",
      "Successfully processed point 1671 (-2.833854, 48.198354)\n",
      "Successfully processed point 2204 (3.056281, 46.46886)\n",
      "Successfully processed point 2410 (-2.852118, 47.522143)\n",
      "Successfully processed point 2586 (-0.317878, 48.826356)\n",
      "No items found for point 513 (88.888888, 88.888888)\n",
      "No data found for point 513 (88.888888, 88.888888) - Attempt 1/3\n",
      "Waiting 13.7 seconds before retry...\n",
      "No items found for point 935 (88.888888, 88.888888)\n",
      "No data found for point 935 (88.888888, 88.888888) - Attempt 1/3\n",
      "Waiting 13.2 seconds before retry...\n",
      "No items found for point 935 (88.888888, 88.888888)\n",
      "No data found for point 935 (88.888888, 88.888888) - Attempt 2/3\n",
      "Waiting 7.1 seconds before retry...\n",
      "No items found for point 513 (88.888888, 88.888888)\n",
      "No data found for point 513 (88.888888, 88.888888) - Attempt 2/3\n",
      "Waiting 8.5 seconds before retry...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing missing points:  88%|████████▊ | 7/8 [00:21<00:03,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No items found for point 935 (88.888888, 88.888888)\n",
      "No data found for point 935 (88.888888, 88.888888) - Attempt 3/3\n",
      "Failed to process point 935 (88.888888, 88.888888) after 3 attempts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing missing points: 100%|██████████| 8/8 [00:22<00:00,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No items found for point 513 (88.888888, 88.888888)\n",
      "No data found for point 513 (88.888888, 88.888888) - Attempt 3/3\n",
      "Failed to process point 513 (88.888888, 88.888888) after 3 attempts\n",
      "Successfully processed 6 out of 8 missing points\n",
      "Updated data saved to data/auxiliary_data_results_full_updated.parquet (2805 total points)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load your original data points\n",
    "target_raw = pd.read_csv('data/France_lab.csv')\n",
    "long_lat = target_raw[['GPS_LONG', 'GPS_LAT']]\n",
    "\n",
    "# Update missing points\n",
    "updated_results = update_missing_dlr_data(\n",
    "    target_raw_path='data/France_lab.csv',\n",
    "    input_parquet=\"data/auxiliary_data_results_full.parquet\",\n",
    "    output_parquet=\"data/auxiliary_data_results_full_updated.parquet\",  # Optional: set to None to overwrite input file\n",
    "    cache_dir=\"data/stac_data_cache_full\",\n",
    "    max_workers=4,\n",
    "    max_retries=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
