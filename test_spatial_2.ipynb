{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Soil Grid Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "\n",
    "# Add a lock to prevent race conditions when saving data\n",
    "save_lock = threading.Lock()\n",
    "\n",
    "def get_soilgrids_point(lon, lat, point_idx, properties=None, max_retries=3):\n",
    "    \"\"\"Get SoilGrids data for a single point with correct field mapping\"\"\"\n",
    "    if properties is None:\n",
    "        properties = ['soc', 'clay', 'sand', 'silt', 'bdod', 'phh2o']\n",
    "        \n",
    "    url = \"https://rest.isric.org/soilgrids/v2.0/properties/query\"\n",
    "    params = {\n",
    "        'lon': lon,\n",
    "        'lat': lat,\n",
    "        'property': properties,\n",
    "        'depth': ['0-5cm', '5-15cm', '15-30cm'],\n",
    "        'value': ['mean']\n",
    "    }\n",
    "    \n",
    "    for retry in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            \n",
    "            # Handle rate limiting\n",
    "            if response.status_code == 429:\n",
    "                wait_time = 15 + random.random() * 15\n",
    "                #print(f\"Rate limited for point {point_idx}, waiting {wait_time:.1f} seconds\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "                \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                \n",
    "                # Start with basic info\n",
    "                result = {'point_index': point_idx, 'lon': lon, 'lat': lat}\n",
    "                \n",
    "                # Extract data using the correct field structure\n",
    "                if 'properties' in data and 'layers' in data['properties']:\n",
    "                    for layer in data['properties']['layers']:\n",
    "                        # Get property name\n",
    "                        prop_name = layer.get('name', 'unknown')\n",
    "                        \n",
    "                        for depth in layer.get('depths', []):\n",
    "                            # Get depth label (which is the string format we need)\n",
    "                            depth_label = depth.get('label', 'unknown')\n",
    "                            \n",
    "                            # Clean the depth label for column naming\n",
    "                            clean_depth = depth_label.replace('-', '_to_')\n",
    "                            \n",
    "                            # Extract values\n",
    "                            for value_type, value in depth.get('values', {}).items():\n",
    "                                column_name = f\"{prop_name}_{clean_depth}_{value_type}\"\n",
    "                                result[column_name] = value\n",
    "                \n",
    "                # Debug print to verify data is being captured correctly\n",
    "                #print(f\"Retrieved data for point {point_idx}: {lon}, {lat}\")\n",
    "                return result\n",
    "            else:\n",
    "                #print(f\"Error for point {point_idx}: Status code {response.status_code}\")\n",
    "                if retry < max_retries - 1:\n",
    "                    wait_time = 10 * (retry + 1)\n",
    "                    #print(f\"Retrying in {wait_time} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    return {'point_index': point_idx, 'lon': lon, 'lat': lat, \n",
    "                            'error': f\"Status {response.status_code}\"}\n",
    "        \n",
    "        except Exception as e:\n",
    "            #print(f\"Exception for point {point_idx}: {str(e)}\")\n",
    "            if retry < max_retries - 1:\n",
    "                wait_time = 10 * (retry + 1)\n",
    "                #print(f\"Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                return {'point_index': point_idx, 'lon': lon, 'lat': lat, \n",
    "                        'error': f\"Exception: {str(e)}\"}\n",
    "    \n",
    "    return {'point_index': point_idx, 'lon': lon, 'lat': lat, \n",
    "            'error': \"Max retries reached\"}\n",
    "\n",
    "def process_point(args):\n",
    "    \"\"\"Wrapper function for concurrent processing\"\"\"\n",
    "    lon, lat, idx, properties = args\n",
    "    # Add jitter to avoid all workers hitting the API simultaneously\n",
    "    time.sleep(random.random() * 2)\n",
    "    return get_soilgrids_point(lon, lat, idx, properties)\n",
    "\n",
    "def save_checkpoint(results, filename, verbose=False):\n",
    "    \"\"\"Save results to a checkpoint file using a lock to prevent race conditions\"\"\"\n",
    "    with save_lock:\n",
    "        try:\n",
    "            df_results = pd.DataFrame(results)\n",
    "            # First write to a temporary file, then rename to avoid partial writes\n",
    "            temp_file = f\"{filename}.temp\"\n",
    "            df_results.to_csv(temp_file, index=False)\n",
    "            os.replace(temp_file, filename)\n",
    "            if verbose:\n",
    "                print(f\"Saved checkpoint with {len(results)} points to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving checkpoint: {str(e)}\")\n",
    "\n",
    "def get_soilgrids_parallel(coordinates_df, num_workers=4, lon_col='GPS_LONG', lat_col='GPS_LAT', \n",
    "                           properties=None, cache_file='soilgrids_parallel.csv',\n",
    "                           checkpoint_interval=10, debug=False):\n",
    "    \"\"\"\n",
    "    Retrieve soil data for multiple points in parallel using multiple workers\n",
    "    \n",
    "    Args:\n",
    "        coordinates_df: DataFrame with coordinates\n",
    "        num_workers: Number of parallel workers (default: 4)\n",
    "        lon_col: Column name for longitude\n",
    "        lat_col: Column name for latitude\n",
    "        properties: List of SoilGrids properties to retrieve\n",
    "        cache_file: Output file name\n",
    "        checkpoint_interval: Save intermediate results every N points\n",
    "        debug: Enable additional debug output\n",
    "    \"\"\"\n",
    "    if properties is None:\n",
    "        properties = ['soc', 'clay', 'sand', 'silt', 'bdod', 'phh2o']\n",
    "    \n",
    "    # Print the input data to verify it's correct\n",
    "    if debug:\n",
    "        print(\"Input coordinate data sample:\")\n",
    "        print(coordinates_df.head())\n",
    "        print(f\"Longitude column: {lon_col}, Latitude column: {lat_col}\")\n",
    "    \n",
    "    # Check for existing cache to resume from\n",
    "    results = []\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        try:\n",
    "            existing_df = pd.read_csv(cache_file)\n",
    "            if len(existing_df) > 0:\n",
    "                results = existing_df.to_dict('records')\n",
    "                processed_indices = set(existing_df['point_index'].unique())\n",
    "                print(f\"Found {len(processed_indices)} already processed points in {cache_file}\")\n",
    "                coordinates_df = coordinates_df[~coordinates_df.index.isin(processed_indices)]\n",
    "                print(f\"Remaining points to process: {len(coordinates_df)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading existing cache: {str(e)}. Starting from scratch.\")\n",
    "    \n",
    "    if len(coordinates_df) == 0:\n",
    "        print(\"All points already processed!\")\n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    # Prepare arguments for parallel processing\n",
    "    args_list = []\n",
    "    for idx, row in coordinates_df.iterrows():\n",
    "        # Verify and clean coordinate values\n",
    "        try:\n",
    "            lon = float(row[lon_col])\n",
    "            lat = float(row[lat_col])\n",
    "            args_list.append((lon, lat, idx, properties))\n",
    "            if debug and len(args_list) <= 5:\n",
    "                print(f\"Prepared point {idx}: lon={lon}, lat={lat}\")\n",
    "        except (ValueError, TypeError) as e:\n",
    "            print(f\"Error with coordinates at index {idx}: {e}\")\n",
    "            print(f\"Row data: {row}\")\n",
    "    \n",
    "    print(f\"Processing {len(args_list)} points with {num_workers} workers\")\n",
    "    \n",
    "    completed_count = 0\n",
    "    \n",
    "    # Use ThreadPoolExecutor for parallel HTTP requests\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_args = {executor.submit(process_point, args): args for args in args_list}\n",
    "        \n",
    "        # Use tqdm for a progress bar\n",
    "        for future in tqdm(concurrent.futures.as_completed(future_to_args), total=len(args_list)):\n",
    "            args = future_to_args[future]\n",
    "            point_idx = args[2]\n",
    "            \n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    results.append(result)\n",
    "                    completed_count += 1\n",
    "                    \n",
    "                    # Save intermediate results periodically\n",
    "                    if completed_count % checkpoint_interval == 0:\n",
    "                        save_checkpoint(results, cache_file)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing point {point_idx}: {str(e)}\")\n",
    "    \n",
    "    # Save final results\n",
    "    save_checkpoint(results, cache_file, verbose=False)\n",
    "    \n",
    "    # Verify the final output\n",
    "    try:\n",
    "        final_df = pd.read_csv(cache_file)\n",
    "        print(f\"Final output has {len(final_df)} rows and {len(final_df.columns)} columns\")\n",
    "        print(\"Column names:\", final_df.columns.tolist())\n",
    "        print(\"First few rows:\")\n",
    "        print(final_df.head())\n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying final output: {str(e)}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example usage:\n",
    "# df = pd.read_csv('coordinates.csv', index_col=0)  # Set the first column as index if that's your point_index\n",
    "# results = get_soilgrids_parallel(df, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 670 already processed points in soilgrids_parallel.csv\n",
      "Remaining points to process: 2137\n",
      "Processing 2137 points with 4 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2137/2137 [1:41:01<00:00,  2.84s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output has 2807 rows and 22 columns\n",
      "Column names: ['point_index', 'lon', 'lat', 'bdod_0_to_5cm_mean', 'bdod_5_to_15cm_mean', 'bdod_15_to_30cm_mean', 'clay_0_to_5cm_mean', 'clay_5_to_15cm_mean', 'clay_15_to_30cm_mean', 'phh2o_0_to_5cm_mean', 'phh2o_5_to_15cm_mean', 'phh2o_15_to_30cm_mean', 'sand_0_to_5cm_mean', 'sand_5_to_15cm_mean', 'sand_15_to_30cm_mean', 'silt_0_to_5cm_mean', 'silt_5_to_15cm_mean', 'silt_15_to_30cm_mean', 'soc_0_to_5cm_mean', 'soc_5_to_15cm_mean', 'soc_15_to_30cm_mean', 'error']\n",
      "First few rows:\n",
      "   point_index       lon        lat  bdod_0_to_5cm_mean  bdod_5_to_15cm_mean  \\\n",
      "0            1  4.584692  45.816720               125.0                134.0   \n",
      "1            0  4.680379  45.893933               128.0                138.0   \n",
      "2            3  4.601575  45.908022               133.0                140.0   \n",
      "3            2  4.671533  45.983716               129.0                139.0   \n",
      "4            6  4.439863  46.224665               102.0                116.0   \n",
      "\n",
      "   bdod_15_to_30cm_mean  clay_0_to_5cm_mean  clay_5_to_15cm_mean  \\\n",
      "0                 141.0               250.0                270.0   \n",
      "1                 141.0               303.0                319.0   \n",
      "2                 144.0               247.0                268.0   \n",
      "3                 143.0               249.0                254.0   \n",
      "4                 121.0               188.0                172.0   \n",
      "\n",
      "   clay_15_to_30cm_mean  phh2o_0_to_5cm_mean  ...  sand_0_to_5cm_mean  \\\n",
      "0                 302.0                 58.0  ...               375.0   \n",
      "1                 338.0                 62.0  ...               263.0   \n",
      "2                 288.0                 60.0  ...               334.0   \n",
      "3                 300.0                 63.0  ...               305.0   \n",
      "4                 205.0                 52.0  ...               505.0   \n",
      "\n",
      "   sand_5_to_15cm_mean  sand_15_to_30cm_mean  silt_0_to_5cm_mean  \\\n",
      "0                366.0                 366.0               375.0   \n",
      "1                243.0                 275.0               434.0   \n",
      "2                320.0                 329.0               418.0   \n",
      "3                295.0                 313.0               446.0   \n",
      "4                514.0                 494.0               306.0   \n",
      "\n",
      "   silt_5_to_15cm_mean  silt_15_to_30cm_mean  soc_0_to_5cm_mean  \\\n",
      "0                364.0                 332.0              489.0   \n",
      "1                438.0                 387.0              467.0   \n",
      "2                412.0                 383.0              401.0   \n",
      "3                451.0                 387.0              422.0   \n",
      "4                314.0                 301.0              786.0   \n",
      "\n",
      "   soc_5_to_15cm_mean  soc_15_to_30cm_mean  error  \n",
      "0               249.0                246.0    NaN  \n",
      "1               254.0                182.0    NaN  \n",
      "2               289.0                153.0    NaN  \n",
      "3               271.0                154.0    NaN  \n",
      "4               620.0                239.0    NaN  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>point_index</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>bdod_0_to_5cm_mean</th>\n",
       "      <th>bdod_5_to_15cm_mean</th>\n",
       "      <th>bdod_15_to_30cm_mean</th>\n",
       "      <th>clay_0_to_5cm_mean</th>\n",
       "      <th>clay_5_to_15cm_mean</th>\n",
       "      <th>clay_15_to_30cm_mean</th>\n",
       "      <th>phh2o_0_to_5cm_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>sand_0_to_5cm_mean</th>\n",
       "      <th>sand_5_to_15cm_mean</th>\n",
       "      <th>sand_15_to_30cm_mean</th>\n",
       "      <th>silt_0_to_5cm_mean</th>\n",
       "      <th>silt_5_to_15cm_mean</th>\n",
       "      <th>silt_15_to_30cm_mean</th>\n",
       "      <th>soc_0_to_5cm_mean</th>\n",
       "      <th>soc_5_to_15cm_mean</th>\n",
       "      <th>soc_15_to_30cm_mean</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4.584692</td>\n",
       "      <td>45.816720</td>\n",
       "      <td>125.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>...</td>\n",
       "      <td>375.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>364.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>246.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4.680379</td>\n",
       "      <td>45.893933</td>\n",
       "      <td>128.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>338.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>...</td>\n",
       "      <td>263.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>275.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>387.0</td>\n",
       "      <td>467.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.601575</td>\n",
       "      <td>45.908022</td>\n",
       "      <td>133.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>334.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>412.0</td>\n",
       "      <td>383.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>4.671533</td>\n",
       "      <td>45.983716</td>\n",
       "      <td>129.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>...</td>\n",
       "      <td>305.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>446.0</td>\n",
       "      <td>451.0</td>\n",
       "      <td>387.0</td>\n",
       "      <td>422.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4.439863</td>\n",
       "      <td>46.224665</td>\n",
       "      <td>102.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td>505.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>494.0</td>\n",
       "      <td>306.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>786.0</td>\n",
       "      <td>620.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2802</th>\n",
       "      <td>2803</td>\n",
       "      <td>5.058028</td>\n",
       "      <td>45.713629</td>\n",
       "      <td>131.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>...</td>\n",
       "      <td>376.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>375.0</td>\n",
       "      <td>429.0</td>\n",
       "      <td>439.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>448.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>232.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2803</th>\n",
       "      <td>2806</td>\n",
       "      <td>4.784826</td>\n",
       "      <td>45.881063</td>\n",
       "      <td>128.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>323.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>522.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>430.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2804</th>\n",
       "      <td>2805</td>\n",
       "      <td>4.381513</td>\n",
       "      <td>45.788303</td>\n",
       "      <td>124.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>...</td>\n",
       "      <td>434.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>366.0</td>\n",
       "      <td>369.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>541.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2805</th>\n",
       "      <td>2800</td>\n",
       "      <td>4.718750</td>\n",
       "      <td>45.498638</td>\n",
       "      <td>119.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>...</td>\n",
       "      <td>467.0</td>\n",
       "      <td>473.0</td>\n",
       "      <td>440.0</td>\n",
       "      <td>364.0</td>\n",
       "      <td>368.0</td>\n",
       "      <td>348.0</td>\n",
       "      <td>545.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2806</th>\n",
       "      <td>2799</td>\n",
       "      <td>4.578846</td>\n",
       "      <td>45.617959</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Max retries reached</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2807 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      point_index       lon        lat  bdod_0_to_5cm_mean  \\\n",
       "0               1  4.584692  45.816720               125.0   \n",
       "1               0  4.680379  45.893933               128.0   \n",
       "2               3  4.601575  45.908022               133.0   \n",
       "3               2  4.671533  45.983716               129.0   \n",
       "4               6  4.439863  46.224665               102.0   \n",
       "...           ...       ...        ...                 ...   \n",
       "2802         2803  5.058028  45.713629               131.0   \n",
       "2803         2806  4.784826  45.881063               128.0   \n",
       "2804         2805  4.381513  45.788303               124.0   \n",
       "2805         2800  4.718750  45.498638               119.0   \n",
       "2806         2799  4.578846  45.617959                 NaN   \n",
       "\n",
       "      bdod_5_to_15cm_mean  bdod_15_to_30cm_mean  clay_0_to_5cm_mean  \\\n",
       "0                   134.0                 141.0               250.0   \n",
       "1                   138.0                 141.0               303.0   \n",
       "2                   140.0                 144.0               247.0   \n",
       "3                   139.0                 143.0               249.0   \n",
       "4                   116.0                 121.0               188.0   \n",
       "...                   ...                   ...                 ...   \n",
       "2802                147.0                 150.0               195.0   \n",
       "2803                137.0                 141.0               181.0   \n",
       "2804                135.0                 137.0               201.0   \n",
       "2805                129.0                 135.0               169.0   \n",
       "2806                  NaN                   NaN                 NaN   \n",
       "\n",
       "      clay_5_to_15cm_mean  clay_15_to_30cm_mean  phh2o_0_to_5cm_mean  ...  \\\n",
       "0                   270.0                 302.0                 58.0  ...   \n",
       "1                   319.0                 338.0                 62.0  ...   \n",
       "2                   268.0                 288.0                 60.0  ...   \n",
       "3                   254.0                 300.0                 63.0  ...   \n",
       "4                   172.0                 205.0                 52.0  ...   \n",
       "...                   ...                   ...                  ...  ...   \n",
       "2802                196.0                 215.0                 63.0  ...   \n",
       "2803                164.0                 204.0                 60.0  ...   \n",
       "2804                193.0                 213.0                 58.0  ...   \n",
       "2805                159.0                 212.0                 58.0  ...   \n",
       "2806                  NaN                   NaN                  NaN  ...   \n",
       "\n",
       "      sand_0_to_5cm_mean  sand_5_to_15cm_mean  sand_15_to_30cm_mean  \\\n",
       "0                  375.0                366.0                 366.0   \n",
       "1                  263.0                243.0                 275.0   \n",
       "2                  334.0                320.0                 329.0   \n",
       "3                  305.0                295.0                 313.0   \n",
       "4                  505.0                514.0                 494.0   \n",
       "...                  ...                  ...                   ...   \n",
       "2802               376.0                366.0                 375.0   \n",
       "2803               323.0                314.0                 344.0   \n",
       "2804               434.0                437.0                 434.0   \n",
       "2805               467.0                473.0                 440.0   \n",
       "2806                 NaN                  NaN                   NaN   \n",
       "\n",
       "      silt_0_to_5cm_mean  silt_5_to_15cm_mean  silt_15_to_30cm_mean  \\\n",
       "0                  375.0                364.0                 332.0   \n",
       "1                  434.0                438.0                 387.0   \n",
       "2                  418.0                412.0                 383.0   \n",
       "3                  446.0                451.0                 387.0   \n",
       "4                  306.0                314.0                 301.0   \n",
       "...                  ...                  ...                   ...   \n",
       "2802               429.0                439.0                 410.0   \n",
       "2803               496.0                522.0                 452.0   \n",
       "2804               366.0                369.0                 353.0   \n",
       "2805               364.0                368.0                 348.0   \n",
       "2806                 NaN                  NaN                   NaN   \n",
       "\n",
       "      soc_0_to_5cm_mean  soc_5_to_15cm_mean  soc_15_to_30cm_mean  \\\n",
       "0                 489.0               249.0                246.0   \n",
       "1                 467.0               254.0                182.0   \n",
       "2                 401.0               289.0                153.0   \n",
       "3                 422.0               271.0                154.0   \n",
       "4                 786.0               620.0                239.0   \n",
       "...                 ...                 ...                  ...   \n",
       "2802              448.0               173.0                232.0   \n",
       "2803              430.0               167.0                187.0   \n",
       "2804              541.0               294.0                260.0   \n",
       "2805              545.0               215.0                212.0   \n",
       "2806                NaN                 NaN                  NaN   \n",
       "\n",
       "                    error  \n",
       "0                     NaN  \n",
       "1                     NaN  \n",
       "2                     NaN  \n",
       "3                     NaN  \n",
       "4                     NaN  \n",
       "...                   ...  \n",
       "2802                  NaN  \n",
       "2803                  NaN  \n",
       "2804                  NaN  \n",
       "2805                  NaN  \n",
       "2806  Max retries reached  \n",
       "\n",
       "[2807 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your data\n",
    "target_raw = pd.read_csv('data/France_lab.csv')\n",
    "long_lat = target_raw[['GPS_LONG', 'GPS_LAT']]\n",
    "get_soilgrids_parallel(long_lat, num_workers=4, properties=['soc', 'clay', 'sand', 'silt', 'bdod', 'phh2o'], debug = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_missing_soilgrids_data(csv_file, output_file=None, max_retries=5, delay_between_retries=20):\n",
    "    \"\"\"\n",
    "    Update missing data in a SoilGrids CSV file\n",
    "    \n",
    "    Args:\n",
    "        csv_file: Path to the CSV file with missing data\n",
    "        output_file: Path to save the updated CSV (default: overwrite input file)\n",
    "        max_retries: Maximum number of retries for failed API calls\n",
    "        delay_between_retries: Delay in seconds between retries\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with the updated data\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    import random\n",
    "    import numpy as np\n",
    "    \n",
    "    if output_file is None:\n",
    "        output_file = csv_file\n",
    "    \n",
    "    # Load the CSV file and force column types\n",
    "    print(f\"Loading data from {csv_file}...\")\n",
    "    df = pd.read_csv(csv_file, header=None)\n",
    "    \n",
    "    # Determine data types for all columns\n",
    "    dtypes = df.dtypes\n",
    "    print(f\"Column data types: {dtypes}\")\n",
    "    \n",
    "    # Identify rows with missing data (rows with mostly empty values)\n",
    "    # Consider both NaN values and empty strings as missing\n",
    "    missing_mask = ((df.iloc[:, 3:].isna()) | (df.iloc[:, 3:] == \"\")).sum(axis=1) > (df.shape[1] - 3) * 0.5\n",
    "    missing_indices = df[missing_mask].index\n",
    "    \n",
    "    print(f\"Found {len(missing_indices)} rows with missing data\")\n",
    "    \n",
    "    if len(missing_indices) == 0:\n",
    "        print(\"No missing data to update!\")\n",
    "        return df\n",
    "    \n",
    "    # Prepare a results list to store updated rows\n",
    "    updated_rows = []\n",
    "    \n",
    "    # Process each row with missing data\n",
    "    for idx in missing_indices:\n",
    "        row = df.iloc[idx]\n",
    "        point_idx = row[0]\n",
    "        lon = row[1]\n",
    "        lat = row[2]\n",
    "        \n",
    "        print(f\"Processing missing data for point {point_idx} at coordinates {lon}, {lat}\")\n",
    "        \n",
    "        # Make API call with retries\n",
    "        for retry in range(max_retries):\n",
    "            try:\n",
    "                result = get_soilgrids_point(lon, lat, point_idx)\n",
    "                \n",
    "                if 'error' in result:\n",
    "                    print(f\"Attempt {retry+1}/{max_retries} failed: {result.get('error')}\")\n",
    "                    \n",
    "                    # If we've reached the max retries, save what we have\n",
    "                    if retry == max_retries - 1:\n",
    "                        print(f\"Failed to update point {point_idx} after {max_retries} attempts\")\n",
    "                        break\n",
    "                    \n",
    "                    # Wait before retrying\n",
    "                    sleep_time = delay_between_retries + random.random() * 10\n",
    "                    print(f\"Retrying in {sleep_time:.1f} seconds...\")\n",
    "                    time.sleep(sleep_time)\n",
    "                    continue\n",
    "                \n",
    "                # Create a new row with the correct data types\n",
    "                updated_row = row.copy()\n",
    "                \n",
    "                # Set the basic fields (point_idx, lon, lat)\n",
    "                # Convert to the same type as the original DataFrame to avoid warnings\n",
    "                updated_row[0] = point_idx  # This should already be the correct type\n",
    "                updated_row[1] = lon        # This should already be the correct type\n",
    "                updated_row[2] = lat        # This should already be the correct type\n",
    "                \n",
    "                # Map the result fields to the appropriate columns in the dataframe\n",
    "                soil_properties = ['soc', 'clay', 'sand', 'silt', 'bdod', 'phh2o']\n",
    "                depths = ['0_to_5cm', '5_to_15cm', '15_to_30cm']\n",
    "                \n",
    "                # Assuming the columns in the original DataFrame follow this order:\n",
    "                column_idx = 3  # Start after point_idx, lon, lat\n",
    "                for prop in soil_properties:\n",
    "                    for depth in depths:\n",
    "                        column_name = f\"{prop}_{depth}_mean\"\n",
    "                        if column_name in result and column_idx < len(df.columns):\n",
    "                            # Try to match the data type\n",
    "                            value = result[column_name]\n",
    "                            if pd.api.types.is_float_dtype(dtypes[column_idx]):\n",
    "                                value = float(value) if value is not None else np.nan\n",
    "                            elif pd.api.types.is_integer_dtype(dtypes[column_idx]):\n",
    "                                value = int(value) if value is not None else np.nan\n",
    "                            updated_row[column_idx] = value\n",
    "                        column_idx += 1\n",
    "                \n",
    "                # Update the DataFrame\n",
    "                df.iloc[idx] = updated_row\n",
    "                print(f\"Successfully updated point {point_idx}\")\n",
    "                \n",
    "                # Add a small delay to avoid rate limiting\n",
    "                time.sleep(2 + random.random() * 3)\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error updating point {point_idx}: {str(e)}\")\n",
    "                \n",
    "                if retry < max_retries - 1:\n",
    "                    sleep_time = delay_between_retries + random.random() * 10\n",
    "                    print(f\"Retrying in {sleep_time:.1f} seconds...\")\n",
    "                    time.sleep(sleep_time)\n",
    "                else:\n",
    "                    print(f\"Failed to update point {point_idx} after {max_retries} attempts\")\n",
    "    \n",
    "    # Save the updated DataFrame\n",
    "    print(f\"Saving updated data to {output_file}...\")\n",
    "    df.to_csv(output_file, index=False, header=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/soilgrids_parallel.csv...\n",
      "Column data types: 0     object\n",
      "1     object\n",
      "2     object\n",
      "3     object\n",
      "4     object\n",
      "5     object\n",
      "6     object\n",
      "7     object\n",
      "8     object\n",
      "9     object\n",
      "10    object\n",
      "11    object\n",
      "12    object\n",
      "13    object\n",
      "14    object\n",
      "15    object\n",
      "16    object\n",
      "17    object\n",
      "18    object\n",
      "19    object\n",
      "20    object\n",
      "21    object\n",
      "dtype: object\n",
      "Found 103 rows with missing data\n",
      "Processing missing data for point 155 at coordinates 0.669178, 49.855175\n",
      "Successfully updated point 155\n",
      "Processing missing data for point 191 at coordinates 2.743706, 48.544291\n",
      "Successfully updated point 191\n",
      "Processing missing data for point 190 at coordinates 2.557593, 48.513858\n",
      "Successfully updated point 190\n",
      "Processing missing data for point 216 at coordinates 3.131174, 48.695394\n",
      "Successfully updated point 216\n",
      "Processing missing data for point 250 at coordinates -0.054073, 46.837853\n",
      "Successfully updated point 250\n",
      "Processing missing data for point 297 at coordinates 2.737896, 49.699723\n",
      "Successfully updated point 297\n",
      "Processing missing data for point 338 at coordinates 5.884201, 48.020614\n",
      "Successfully updated point 338\n",
      "Processing missing data for point 365 at coordinates 4.987468, 44.229435\n",
      "Successfully updated point 365\n",
      "Processing missing data for point 398 at coordinates 1.13659, 43.97067\n",
      "Successfully updated point 398\n",
      "Processing missing data for point 397 at coordinates 1.33483, 44.15072\n",
      "Successfully updated point 397\n",
      "Processing missing data for point 396 at coordinates 1.4312, 44.17679\n",
      "Successfully updated point 396\n",
      "Processing missing data for point 451 at coordinates -4.271489, 47.855326\n",
      "Successfully updated point 451\n",
      "Processing missing data for point 466 at coordinates 0.381898, 46.769025\n",
      "Successfully updated point 466\n",
      "Processing missing data for point 491 at coordinates 0.237318, 46.266361\n",
      "Successfully updated point 491\n",
      "Processing missing data for point 513 at coordinates 88.888888, 88.888888\n",
      "Successfully updated point 513\n",
      "Processing missing data for point 511 at coordinates 3.00959, 48.2002\n",
      "Successfully updated point 511\n",
      "Processing missing data for point 512 at coordinates 3.18728, 48.08494\n",
      "Successfully updated point 512\n",
      "Processing missing data for point 555 at coordinates 2.046038, 48.134749\n",
      "Successfully updated point 555\n",
      "Processing missing data for point 574 at coordinates 6.0446, 49.39529\n",
      "Successfully updated point 574\n",
      "Processing missing data for point 605 at coordinates 1.121382, 45.981392\n",
      "Successfully updated point 605\n",
      "Processing missing data for point 622 at coordinates 1.139869, 43.299735\n",
      "Successfully updated point 622\n",
      "Processing missing data for point 627 at coordinates 2.207169, 46.193274\n",
      "Successfully updated point 627\n",
      "Processing missing data for point 668 at coordinates -1.359804, 47.234471\n",
      "Successfully updated point 668\n",
      "Processing missing data for point 695 at coordinates -0.3349, 45.15926\n",
      "Successfully updated point 695\n",
      "Processing missing data for point 722 at coordinates 1.347223, 43.571493\n",
      "Successfully updated point 722\n",
      "Processing missing data for point 759 at coordinates 4.11086, 43.98437\n",
      "Successfully updated point 759\n",
      "Processing missing data for point 762 at coordinates 4.80933, 43.98572\n",
      "Successfully updated point 762\n",
      "Processing missing data for point 788 at coordinates 0.089928, 43.929543\n",
      "Successfully updated point 788\n",
      "Processing missing data for point 787 at coordinates 0.870324, 43.911071\n",
      "Successfully updated point 787\n",
      "Processing missing data for point 838 at coordinates 3.1851, 43.36619\n",
      "Successfully updated point 838\n",
      "Processing missing data for point 856 at coordinates 5.092115, 47.879903\n",
      "Successfully updated point 856\n",
      "Processing missing data for point 904 at coordinates -1.875838, 48.306003\n",
      "Successfully updated point 904\n",
      "Processing missing data for point 935 at coordinates 88.888888, 88.888888\n",
      "Successfully updated point 935\n",
      "Processing missing data for point 923 at coordinates 6.510341, 48.672605\n",
      "Successfully updated point 923\n",
      "Processing missing data for point 1012 at coordinates 5.384717, 47.009183\n",
      "Successfully updated point 1012\n",
      "Processing missing data for point 1031 at coordinates 0.98026, 47.88937\n",
      "Successfully updated point 1031\n",
      "Processing missing data for point 1057 at coordinates 1.77986, 47.60863\n",
      "Successfully updated point 1057\n",
      "Processing missing data for point 1056 at coordinates 1.106791, 47.62817\n",
      "Successfully updated point 1056\n",
      "Processing missing data for point 1081 at coordinates 4.05244, 45.73408\n",
      "Successfully updated point 1081\n",
      "Processing missing data for point 1110 at coordinates 4.31513, 45.188658\n",
      "Successfully updated point 1110\n",
      "Processing missing data for point 1244 at coordinates 5.967521, 48.996453\n",
      "Successfully updated point 1244\n",
      "Processing missing data for point 1246 at coordinates 5.634411, 45.251803\n",
      "Successfully updated point 1246\n",
      "Processing missing data for point 1269 at coordinates 1.48663, 46.78984\n",
      "Successfully updated point 1269\n",
      "Processing missing data for point 1280 at coordinates 1.17236, 46.6196\n",
      "Successfully updated point 1280\n",
      "Processing missing data for point 1272 at coordinates 1.35562, 47.10523\n",
      "Successfully updated point 1272\n",
      "Processing missing data for point 1333 at coordinates -0.95724, 43.623243\n",
      "Successfully updated point 1333\n",
      "Processing missing data for point 1351 at coordinates -0.239878, 43.806771\n",
      "Successfully updated point 1351\n",
      "Processing missing data for point 1361 at coordinates 2.819498, 44.916088\n",
      "Successfully updated point 1361\n",
      "Processing missing data for point 1399 at coordinates 4.054361, 48.730593\n",
      "Successfully updated point 1399\n",
      "Processing missing data for point 1396 at coordinates 4.300754, 48.959924\n",
      "Successfully updated point 1396\n",
      "Processing missing data for point 1418 at coordinates -0.757008, 48.058626\n",
      "Successfully updated point 1418\n",
      "Processing missing data for point 1465 at coordinates -1.305934, 48.566394\n",
      "Successfully updated point 1465\n",
      "Processing missing data for point 1493 at coordinates 1.564248, 48.749983\n",
      "Successfully updated point 1493\n",
      "Processing missing data for point 1515 at coordinates 0.879868, 48.297504\n",
      "Successfully updated point 1515\n",
      "Processing missing data for point 1513 at coordinates 1.966878, 48.291714\n",
      "Successfully updated point 1513\n",
      "Processing missing data for point 1507 at coordinates 1.600771, 48.047341\n",
      "Successfully updated point 1507\n",
      "Processing missing data for point 1546 at coordinates 5.05454, 44.32302\n",
      "Successfully updated point 1546\n",
      "Processing missing data for point 1549 at coordinates 5.13098, 44.30854\n",
      "Successfully updated point 1549\n",
      "Processing missing data for point 1575 at coordinates 6.666943, 47.128374\n",
      "Successfully updated point 1575\n",
      "Processing missing data for point 1576 at coordinates 6.559288, 47.161011\n",
      "Successfully updated point 1576\n",
      "Processing missing data for point 1645 at coordinates -3.255429, 48.239248\n",
      "Successfully updated point 1645\n",
      "Processing missing data for point 1648 at coordinates -2.951216, 48.439631\n",
      "Successfully updated point 1648\n",
      "Processing missing data for point 1751 at coordinates -0.345346, 46.0298\n",
      "Successfully updated point 1751\n",
      "Processing missing data for point 1773 at coordinates -0.514265, 45.559151\n",
      "Successfully updated point 1773\n",
      "Processing missing data for point 1851 at coordinates 0.21301, 45.683683\n",
      "Successfully updated point 1851\n",
      "Processing missing data for point 1829 at coordinates 0.554783, 45.950545\n",
      "Successfully updated point 1829\n",
      "Processing missing data for point 1898 at coordinates -0.246134, 49.141341\n",
      "Successfully updated point 1898\n",
      "Processing missing data for point 1902 at coordinates 0.091428, 49.117383\n",
      "Successfully updated point 1902\n",
      "Processing missing data for point 1930 at coordinates 4.792664, 43.894531\n",
      "Successfully updated point 1930\n",
      "Processing missing data for point 1938 at coordinates 4.664076, 43.399791\n",
      "Successfully updated point 1938\n",
      "Processing missing data for point 1944 at coordinates 4.764023, 43.386851\n",
      "Successfully updated point 1944\n",
      "Processing missing data for point 1957 at coordinates 3.11161, 44.19392\n",
      "Successfully updated point 1957\n",
      "Processing missing data for point 2079 at coordinates 1.59067, 49.07755\n",
      "Successfully updated point 2079\n",
      "Processing missing data for point 2083 at coordinates 4.7675, 49.721748\n",
      "Successfully updated point 2083\n",
      "Processing missing data for point 2134 at coordinates 7.164566, 43.716186\n",
      "Successfully updated point 2134\n",
      "Processing missing data for point 2204 at coordinates 3.056281, 46.46886\n",
      "Successfully updated point 2204\n",
      "Processing missing data for point 2239 at coordinates 3.509173, 49.764728\n",
      "Successfully updated point 2239\n",
      "Processing missing data for point 2241 at coordinates 3.408321, 49.686713\n",
      "Successfully updated point 2241\n",
      "Processing missing data for point 2274 at coordinates 5.658703, 46.280918\n",
      "Successfully updated point 2274\n",
      "Processing missing data for point 2287 at coordinates 5.371121, 45.962301\n",
      "Successfully updated point 2287\n",
      "Processing missing data for point 2284 at coordinates 5.670066, 45.776498\n",
      "Successfully updated point 2284\n",
      "Processing missing data for point 2310 at coordinates -4.464553, 48.524331\n",
      "Successfully updated point 2310\n",
      "Processing missing data for point 2339 at coordinates -3.909861, 48.322179\n",
      "Successfully updated point 2339\n",
      "Processing missing data for point 2362 at coordinates 5.971293, 48.652804\n",
      "Successfully updated point 2362\n",
      "Processing missing data for point 2366 at coordinates 1.69355, 48.95858\n",
      "Successfully updated point 2366\n",
      "Processing missing data for point 2415 at coordinates -3.147249, 47.723656\n",
      "Successfully updated point 2415\n",
      "Processing missing data for point 2414 at coordinates -3.286465, 47.943738\n",
      "Successfully updated point 2414\n",
      "Processing missing data for point 2526 at coordinates 3.153084, 50.518051\n",
      "Successfully updated point 2526\n",
      "Processing missing data for point 2532 at coordinates 3.137133, 50.609016\n",
      "Successfully updated point 2532\n",
      "Processing missing data for point 2522 at coordinates 3.191674, 50.233321\n",
      "Successfully updated point 2522\n",
      "Processing missing data for point 2548 at coordinates 2.643136, 49.224024\n",
      "Successfully updated point 2548\n",
      "Processing missing data for point 2560 at coordinates 2.155684, 49.335503\n",
      "Successfully updated point 2560\n",
      "Processing missing data for point 2607 at coordinates 0.703044, 48.518116\n",
      "Successfully updated point 2607\n",
      "Processing missing data for point 2661 at coordinates 1.688209, 50.603511\n",
      "Successfully updated point 2661\n",
      "Processing missing data for point 2685 at coordinates 3.534318, 45.758041\n",
      "Successfully updated point 2685\n",
      "Processing missing data for point 2667 at coordinates 3.862523, 45.361973\n",
      "Successfully updated point 2667\n",
      "Processing missing data for point 2671 at coordinates 3.47058, 45.428731\n",
      "Successfully updated point 2671\n",
      "Processing missing data for point 2689 at coordinates 3.689698, 45.532144\n",
      "Successfully updated point 2689\n",
      "Processing missing data for point 2731 at coordinates 2.16721, 44.30777\n",
      "Successfully updated point 2731\n",
      "Processing missing data for point 2737 at coordinates 0.099172, 43.294687\n",
      "Successfully updated point 2737\n",
      "Processing missing data for point 2778 at coordinates 7.667896, 48.900893\n",
      "Successfully updated point 2778\n",
      "Processing missing data for point 2791 at coordinates 7.443721, 47.962394\n",
      "Successfully updated point 2791\n",
      "Processing missing data for point 2799 at coordinates 4.578846, 45.617959\n",
      "Successfully updated point 2799\n",
      "Saving updated data to data/soilgrids_updated.csv...\n"
     ]
    }
   ],
   "source": [
    "# Update the missing data\n",
    "updated_df = update_missing_soilgrids_data(\n",
    "    csv_file='data/soilgrids_parallel.csv',\n",
    "    output_file='data/soilgrids_updated.csv',\n",
    "    max_retries=5,\n",
    "    delay_between_retries=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
